\section{Crawling\label{crawling}}
In order to crawl large amounts of web pages fast, a
distributed system of crawlers was set up, along with
automation scripts written in {\tt BASH}. The scripts allowed
machines to be added to the pool of `crawlers'.  Files were shuttled
to a master machine using {\tt rsync} periodically, and various
cleanup scripts were used in order to ensure that crawler
instances retained a certain level of available disk space.  This
approach was was inspired by distributed crawling method outlined by
\citeA{page1998}.

One difference is that
\nr{} relies heavily on Redis, whereas \citeauthor{page1998} write
about a custom URLServer that was designed from scratch.
Another key difference is
the method of distributing and storing files.  Rather than some
distributed, fault-tolerant filesystem, \nr{} simply stores all the
compressed web pages on a single RAID-1 disk.  This design
choice was made for three reasons.  Firstly, the task of this
project is limited to recent news, so not as much disk storage is
needed, as compared to a tool for the general web.  Secondly,
disk space has advanced a lot since Google was launched;
renting a dedicated server with 4TB of disk space now costs
as low as â‚¬33 a month\footnote{Price estimated from Hetzner's server auction \url{https://www.hetzner.com/sb}, 2020}.  Lastly, setting up a distributed filesystem
is costly, requires time, specialised expertise, and ties the
project to `the cloud', effectively ruling out the possibility of
development without an internet connection.
Relying on an
old-fashioned single hard drive allowed \nr{} to be developed fast,
and at times without a connection to the internet.