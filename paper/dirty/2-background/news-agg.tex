\section{News aggregation}
Newsreaders and aggregators have been around for quite some time
now.  The RSS specification first emerged in 1999 \cite{rss1999},
and quickly grew in popularity, eventually becoming the key
ingredient in the now defunct {\it Google Reader}.

From the experiences outlined in Section \ref{crawling}, RSS today seems to
be a poor source for the task of ``high-coverage news
crawling'', the process by which large amount good quality
news data are extracted from the public web.

The style of such legacy news aggregation lives on in some way
in today's popular news feeds, including the Twitter and
Facebook timelines, as well as in multimodal media platforms such as
YouTube.  On these platforms, a sequential lists of news items are
presented to users, and to some extent, the user chooses what they see,
by `following' or `liking' a news source.  But the choice of items now
follows a new methodology, and there are clear differences between the
legacy {\it RSS feeds} approach, and the current algorithmic approach.

There is a tradeoff.  Previously, users had granular control over
what they would read, and what they would not read, but the cost
was the time and effort it took to amass a collection of RSS feeds
that was well curated to the user.  Depending on the user's choice
of newsreader, there may have not been additional filter settings.
Some newsreaders had powerful keyword functionality, that could allow
a user, for example, to block all COVID-19 news on Sundays.

Now, users may not need to
spend as much time setting up their own curation service, or
manually navigating to different sites intermittently throughout
the day, in order to read the news.  It's highly automated.  A news
source they never heard of before may appear before them on their
social media feed.  It could be a welcome addition to their news
diet.  On the other side, if a news article is boring or unimportant,
a user may be less likely to see it now, since its appearance on the
`timeline' is often based on its overall performance in the community
(numbers of likes, karma, etc.).

The shift from the old news landscape to the new news landscape has
created new problems, however.  There is growing concern over user
privacy, leading to the introduction of GDPR laws in Europe \cite{gdpr}.
The same machine learning features used to get user news feeds
so `right' also happens to be very valuable for spurious ad campaigns.

In \citeyearNP{page1998}, \citeauthor{page1998} argued for general
search engine development to be pushed into ``the academic realm''.  They
stated concerns over the influence of advertising on quality search
results.  But google's search service can no longer truly be described as
``in the academic realm''.  Its new inner workings are now a business
secret, protected by proprietary software licenses. Furthermore,
{\it Google News}, the news
aggregator launched in 2002, has never really been in the academic
realm, except for the release of a large dataset.  With this in mind,
another motivation for developing \nr{} was to provide
a news aggregation service within the academic realm.  The algorithms
used, and software developed, are all open-sourced, and accessible
on the public git repository: \url{https://github.com/sean-healy/newsreduce/}.  The code
repository includes {\tt BASH} automation
scripts intended to allow others to set up their own NewsReduce
servers.  The architecture is modular, so users may be interested in
using the web crawler alone, or the various parsers, or even the entire
system. As a diclaimer, some technical knowledge in Linux and
{\tt BASH} are needed to successfully launch a NewsReduce instance.