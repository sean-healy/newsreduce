\section{The open corpus problem}

A key topic in the area of news aggregation is the open
corpus problem, as outlined by \cite{henze2001},
\citeA{brusilovsky2007} and  \citeA{koidl}. \citeA{henze2001}
encountered the problem from the domain of e-learning
software, but others have noted the complexity of an
{\it open-ended} corpus, albeit with different terminology.
In the much cited paper of \citeA{page1998}, for example,
large difficulty is mentioned with regards ``unstructured''
web content, and the task of gradually crawling and storing
it.  Some noted problems include the tendency of websites to
break, returning 400 or 500 errors.  The former error code
corresponds with the broader issue of {\it link rot}.
\citeA{markwell2003} studied the demise of ``URL viability''
over the course of 25 months in the area of
biochemistry and molecular biology education, and found that
27.5\% of links were lost over that period (47.9\% for .com
URLs).

A naive solution would be to discard a resource immediately
after the webpage begins to return an error code, but this
would erroneously discard resources from websites with
short, intermittent errors.

Another difficulty arises from tendency of websites to have
vastly different {\tt HTML} patterns.  News sources will often correctly
wrap headlines in {\tt H1} tags, wrap author names in the
appropriate metatags, times and dates in the correct {\tt TIME} tag.
But some will wrap the headline immediately within a {\tt DIV},
others will place each paragraph within separate {\tt SECTION} tags, and
so on. The possibilities for {\tt HTML5} misuse are vast, especially
as a result of {\tt CSS}, and the web developers' ability to
simulate the expected functionality of one {\tt HTML} tag using
{\tt CSS3} rules applied to different {\tt HTML} tag.

Some of the issues \citeA{page1998} faced are no longer as
relevant today.  Parsing syntactically invalid {\tt HTML} is
now trivial, given the wide availability of robust web scraping
libraries in popular programming environments.
\nr{} uses the {\tt JSDOM} library (Section \ref{repr}).  Another
issue that is far less daunting is network latency. With image and
{\tt AJAX}-style content loading disabled, popular websites
appear to load much faster today than in the 90s, and
the web crawling task has a smaller bottleneck as a result.

The huge lengths \citeA{page1998} went to in order to optimise
their implementation of a search engine for sequential disk
acess seems less relevant now, given the widespread adoption of solid
state drives, and huge increases in random access memory in today's
servers.  Sequential access still provides a performance boost, but
in certain areas of the application it may be more worthwhile to
rely on the filesystem for efficiency.  This was the reasoning behind
\nr{}'s storage methodology: one file per resource version format,
and each group of resource version stored within a compressed archive
(Section \ref{compress}).

All that said, there are new issues relating to crawling in today's web.
Some websites have adopted {\it single-page application} (SPA)
architectures, which involve initially loading a web page
with a small to medium sized chunk of JavaScript, but no
actual content.  The chunk of JavaScript code is then responsible
for fetching individual pieces of content, laying them out in
the browser, and updating parts of the display if necessary.
SPAs and the originally underlying technology ({\tt AJAX}) have
presented a problem to web crawling for over a decade at the time
of writing \cite{matter2008}, \cite{mesbah2012}.

There are documented solutions to crawling non-static pages, but
these solutions rely on allowing JavaScript code to execute in
a virtual environment, waiting some amount of time, and
then taking a snapshot of the document's DOM structure.
JavaScript is of course Turing complete, so this approach is
cumbersome, and without a rendering time limit, the problem is
actually {\it undecidable}.  This has led to sites like LinkedIn and
Twitter becoming difficult to crawl without strong processing
power. At the time of writing, inspecting the HTML source of arbitrary
LinkedIn and Twitter webpages (via Firefox) reveals no textual
similarities to the eventual text that a user would see.

Fortunately, the largest online news sources that aren't social
networks tend to stick with static {\tt HTML}, but there is no
guarantee that will always be the case, and detecting if a web
resource involves SPA architecture in any form adds a layer of
complexity to the task of crawling the web.  For the purposes of
this thesis, the problem is set to the side, and only static
{\tt HTML} is considered.

The web of today plays a default role in the dissemination of infromation.
Information retrieval papers used to regularly cite the growth rate of the
web \cite{page1998}, \cite{mcbryan1994}. But today it's hardly worth the
trouble, since the vast majority of news sources and businesses are now on the
web.  In fact, finding news sources that are limited to an offline
audience now appears to be the harder task.  This ubiquity
is beneficial for corpus construction, as there is a lot more
data available.  But it is now significantly harder to verify
sources.  The term {\it fake news} has been in the public consciousness
and parlance since approximately 2016\footnote{Google
Trends data observed in August 2020}.  The apparent rise of
{\it fake news} has now been studied.  Researchers have even
tried to solve the problem using various data mining
techniques \cite{shu2017}.

All of the problems discussed represent a tricky open-corpus,
far beyond the complexity of building ML models for a large but
static data set.  In the best case, the \nr{} crawler can go days
without a problem, though in earlier versions, many bugs would
arise, stopping the crawler after a few minutes or a few hours
(Section \ref{crawling}).