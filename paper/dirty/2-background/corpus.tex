\section{The open corpus problem\label{corpus}}

A common difficulty in applications using web data (such as
news aggregation) is the open
corpus problem \cite{henze2001, brusilovsky2007, koidl}.
\citeauthor{henze2001}
encountered the problem from the domain of e-learning
software, but others have noted the complexity of an
{\it open-ended} corpus, albeit with different terminology.
In the much cited paper of \citeA{page1998}, for example,
large difficulty is mentioned with regards ``unstructured''
web content, and the task of gradually crawling and storing
it.  Some noted problems include the tendency of websites to
break, returning 400 or 500 errors.  One of the former error codes
(404) corresponds with the broader issue of {\it link rot}.
\citeA{markwell2003} studied the demise of ``URL viability''
over the course of 25 months in the area of
biochemistry and molecular biology education, and found that
27.5\% of links were lost over that period (47.9\% for .com
URLs).

A naive solution would be to discard a resource immediately
after the webpage begins to return an error code, but this
would erroneously discard resources from websites with
short, intermittent errors.

Another difficulty arises from tendency of websites to have
vastly different {\tt HTML} patterns.  News sources will often correctly
wrap headlines in {\tt H1} tags, wrap author names in the
appropriate metatags, times and dates in the correct {\tt TIME} tag.
But some will wrap the headline immediately within a {\tt DIV},
others will place each paragraph within separate {\tt SECTION} tags, and
so on. The possibilities for {\tt HTML5} misuse are vast, especially
as a result of {\tt CSS}, and the web developers' ability to
simulate the expected functionality of one {\tt HTML} tag using
{\tt CSS3} rules applied to different {\tt HTML} tag.

Some of the corpus-related issues \citeA{page1998} faced are no longer as
relevant today.  Parsing syntactically invalid {\tt HTML} is
now trivial, given the wide availability of robust web scraping
libraries in popular programming environments.
\nr{} uses the {\tt JSDOM} library (Section \ref{parse}).  Another
issue that is far less daunting is network latency. If image and
{\tt AJAX}-style content loading are disabled, websites
tend to load much faster today than in the 90s.  As a result,
the web crawling task has a smaller bottleneck.

The huge lengths \citeA{page1998} went to in order to optimise
their implementation for sequential disk
access seem less valuable now, given the widespread adoption of solid
state drives, and huge increases in random access memory on today's
business and commodity servers.  Sequential access still provides
a performance boost, but in certain areas of the application it may
be more worthwhile to rely on the filesystem or RDBMS for efficiency.  This was
the reasoning behind \nr{}'s storage methodology: one file per resource
version format, and each group of resource versions stored within a
compressed archive (Section \ref{compress}).

All that said, there are new issues relating to crawling today's web.
Some websites have adopted {\it single-page application} (SPA)
architectures, which involve initially loading a web page
with a small to medium sized chunk of JavaScript, but no
actual content.  The chunk of JavaScript code is then responsible
for fetching individual pieces of content, laying them out in
the browser, and updating parts of the display if necessary.
SPAs and the originally underlying technology ({\tt AJAX}) have
presented a problem to web crawling for over a decade at the time
of writing \cite{matter2008, mesbah2012}.

There are documented solutions to crawling non-static pages, but
these solutions rely on allowing JavaScript code to execute in
a virtual environment, waiting some amount of time, and
then taking a snapshot of the document's DOM structure.
JavaScript is of course Turing complete, so this approach is
cumbersome, and without a rendering time limit, the problem is
actually undecidable.  This has led to sites like LinkedIn and
Twitter becoming difficult to crawl without strong processing
power. At the time of writing, inspecting the HTML source of arbitrary
LinkedIn and Twitter webpages (via Firefox) reveals no textual
similarities to the eventual text that a user would see.

Fortunately, the largest online news sources that aren't social
networks tend to stick with static {\tt HTML} (Section \ref{crawling}),
but there is no
guarantee that will always be the case, and detecting if a web
resource involves SPA architecture in any form adds a layer of
complexity to the task of crawling the web.  For the purposes of
this dissertation, the problem is set to the side, and only static
{\tt HTML} is considered.

\subsection{The Web}

The web at the time of writing plays a default role in the dissemination of infromation.
Information retrieval papers used to regularly cite the growth rate of the
web \cite{mcbryan1994}. But today it's hardly worth the
trouble, since the vast majority of news sources and businesses are now on the
web, and have been for quite some time.  In fact, finding news sources that are limited to an offline
audience could now be a more difficult task.  This ubiquity
is beneficial for corpus construction, as there is a lot more
data available.  But it is now significantly harder to verify
sources.  The term {\it fake news} has been in the public consciousness
and parlance since approximately 2016\footnote{Google
Trends data observed in August 2020}.  Its documented rise
has led machine learning researchers to attempts
at the problem \cite{shu2017}.  As with the issue of non-HTML
content, this problem was not a priority while designing \nr{}.
Although, the system does leverage ranking algorithms strongly,
which could cause the incidence of fake news to be less
probable.

All of the problems discussed represent a tricky open-corpus,
and this background literature was taken into consideration
while building the crawler.  Traditionally, data scientists work
with static data sets, fixed in size, and assembled before
programming begins.  Highly performant models, in terms of
precision and recall, can be developed using a static data
set.  But these models are difficult to transition into a
production pipeline, where the dataset is no longer static.